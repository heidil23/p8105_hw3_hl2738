---
title: "P8105 Homework 3"
author: Heidi Lumish
output: github_document
---

```{r setup, include=FALSE}
library(p8105.datasets)
library(tidyverse)
library(ggplot2)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

#### Load the Instacart dataset

First we will load in the Instacart data from the p8105.datasets library.

```{r}
data("instacart")
```

#### Summary of the Instacart dataset

Next, we will summarize the dataset.

The Instacart dataset provides detailed information on grocery orders in 2017. The dataset has `r ncol(instacart)` variables, and it describes `r nrow(instacart)` products from `r length(unique(pull(instacart, user_id)))` unique users. `r round(sum(pull(instacart, reordered))/nrow(instacart)*100, digits = 2)`% products had been ordered by the user in the past. The median time since the prior order for each customer was `r median(pull(instacart, "days_since_prior_order"))` (IQR `r quantile(pull(instacart, "days_since_prior_order"), 0.25)`, `r quantile(pull(instacart, "days_since_prior_order"), 0.75)`) days. There were `r length(unique(pull(instacart, product_name)))` unique products ordered from `r length(unique(pull(instacart, aisle_id)))` different aisles.

### Specifics of the Instacart dataset

#### How many aisles are there, and which aisles are the most items ordered from?

There are `r length(unique(pull(instacart, aisle_id)))` aisles. Using the following code chunk, we can determine that the most items are ordered from the aisles fresh vegetables, fresh fruits, packaged vegetables, yogurt, and packaged cheese.

```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarize(
    aisle_count = n()
  ) %>% 
  slice_max(aisle_count, n = 5)
```

#### Plot showing the number of items ordered in each aisle

Next we will make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered.

```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarize(
    aisle_count = n()
  ) %>% 
  filter(aisle_count >10000) %>% 
  ggplot(aes(x = reorder(aisle, aisle_count), y = aisle_count)) +
  geom_col(width = 0.7) +
  coord_flip() + 
  labs(
    title = "Number of products ordered per aisle",
    x = "Aisle name",
    y = "Number of products")
```

This plot again demonstrates that the most items are ordered from the aisles mentioned above.

#### Table of three most popular items
Now we will make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Included in the table is the number of times each item is ordered. 
```{r}
instacart %>% 
  filter(
    aisle == "baking ingredients" |
      aisle == "dog food care" |
      aisle == "packaged vegetables fruits") %>%
  group_by(product_name) %>% 
  mutate(
    product_count = n()) %>%
  select(aisle, product_name, product_count) %>% 
  distinct() %>% 
  group_by(aisle) %>%
  slice_max(product_count, n = 3) %>% 
  mutate(
    Rank = order(product_count, decreasing = TRUE)
  ) %>%
  unite("product", product_name, product_count, sep = ": ") %>% 
  mutate(
    aisle = str_to_title(aisle)
  ) %>% 
  pivot_wider(
    names_from = "aisle",
    values_from = "product"
    ) %>% 
  knitr::kable(format = "simple",
               caption = "Top three most popular items by aisle with number of items ordered")
```

We can see that the top items ordered are light brown sugar for baking ingredients, snack sticks for dog food care, and organic baby spinach for packaged vegetables and fruits.

#### Pink Lady Apples and Coffee Ice Cream

The following is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r}
instacart %>% 
  filter(
    product_name == "Pink Lady Apples" |
    product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(
    mean_hour = round(mean(order_hour_of_day), digits = 1)
  ) %>% 
  mutate(Day = case_when(
    order_dow == 0 ~ "Sunday",
    order_dow == 1 ~ "Monday",
    order_dow == 2 ~ "Tuesday",
    order_dow == 3 ~ "Wednesday",
    order_dow == 4 ~ "Thursday",
    order_dow == 5 ~ "Friday",
    order_dow == 6 ~ "Saturday")
  ) %>% 
  pivot_wider(
    names_from = "product_name",
    values_from = "mean_hour"
  ) %>% 
  select(-order_dow) %>% 
    knitr::kable(format = "simple",
                 caption = "Mean hour for ordering products by day of the week")
```

We can see that overall, on average, coffee ice cream is ordered later in the day than Pink Lady apples.

## Problem 2

### Load and clean the BRFSS dataset

First we will load the BRFSS dataset from the p8105.datasets package.

```{r}
data("brfss_smart2010")
```

Next we will clean the data. This includes limiting the data set to the "Overall Health" topic, including only responses from “Excellent” to “Poor," and organizing responses as a factor taking levels ordered from “Poor” to “Excellent.”

```{r}
brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  mutate(
    state = locationabbr,
    county = locationdesc
  ) %>% 
  select(-locationabbr, -locationdesc) %>% 
  relocate(year, state, county) %>% 
  filter(topic == "Overall Health") %>% 
  filter(response %in% c("Poor", "Fair", "Good", "Very good", "Excellent")) %>%
  mutate(
    response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))
  )
```

### BRFSS dataset questions

#### Observed at 7 or more locations
In 2002, which states were observed at 7 or more locations? What about in 2010?

Based on the code chunk below, we can see that the there were 6 states (CT, FL, MA, NC, NJ, and PA) that were observed at 7 or more locations in 2002. By comparison, in 2010, there 14 states (CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA) that were observed at 7 or more locations.

```{r}
brfss %>% 
  filter(year == "2002") %>%
  select(state, county) %>% 
  distinct() %>% 
  group_by(state) %>% 
  summarize(count = n()) %>% 
  filter(count >= 7)

brfss %>% 
  filter(year == "2010") %>%
  select(state, county) %>% 
  distinct() %>% 
  group_by(state) %>% 
  summarize(count = n()) %>% 
  filter(count >= 7)
```

#### Excellent responses
We will now construct a dataset that is limited to *Excellent* responses and contains year, state, and a variable that averages the data_value across locations within a state. We will then make a “spaghetti” plot of this average value over time within a state.

```{r}
excellent = brfss %>% 
  filter(response == "Excellent") %>% 
  group_by(state, year) %>% 
  mutate(
    mean_data_value = mean(data_value)
  ) %>% 
  select(year, state, mean_data_value)

excellent %>% 
  ggplot(aes(x = year, y = mean_data_value, color = state)) +
  geom_point(size = .2) + 
  geom_line(aes(group = state), alpha = .3) +
  labs(
    title = "Average data value over time for Excellent responses",
    x = "Year",
    y = "Mean data value")
```

From the spaghetti plot, we can see that overall, the average data value tended to decrease over time from 2002 to 2010.

#### Distribution of responses in 2006 and 2010 in NY state
We will now filter the data to include the years 2006 and 2010 only and limit the dataset to NY state. The box plot below shows the distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r}
brfss %>% 
  filter(
    year == c("2006", "2010"),
    state == "NY") %>% 
  ggplot(aes(x = response, y = data_value, color = response)) +
  geom_boxplot() + 
  facet_grid(.~year) +
  labs(
    title = "Distribution of data values by response in NY",
    x = "Response",
    y = "Data Value")
```

These box plots plot show that a response of "Poor" correlated with the lowest data values, both in 2006 and 2010. A response of "Fair" similarly correlated with lower data values, though not quite as low as for "Poor." In 2006, there was overlap between the data values for "Good," "Very good," and "Excellent" responses. In 2010, the highest data values coresponded with "Good" and "Very good" responses, whereas data values for "Excellent" were slightly lower and were more similar to the values in 2006.

## Problem 3
This problem uses five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). The variables include the activity counts for each minute of a 24-hour day starting at midnight.

#### Load and tidy the accelerometer data
First we will load, tidy, and wrangle the accelerometer data. We will add a variable specifying "weekday" vs. "weekend," and we will use pivot_longer to create a variable for minute of the day and a separate variable for the activity counts.

```{r}
accel = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    day_type = case_when(
      day == "Sunday" ~ "weekend",
      day == "Monday" ~ "weekday",
      day == "Tuesday" ~ "weekday",
      day == "Wednesday" ~ "weekday",
      day == "Thursday" ~ "weekday",
      day == "Friday" ~ "weekday",
      day == "Saturday" ~ "weekend"),
    week = as.numeric(as.character(week))
  ) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity",
    names_prefix = "activity_",
    values_to = "counts"
  ) %>% 
  mutate(counts = as.numeric(counts))
```

#### Summary of the dataset

The tidied accelerometer dataset has `r ncol(accel)` variables and `r nrow(accel)` observations. It includes data from week `r min(pull(accel, week))` to week `r max(pull(accel, week))` of the study. The dataset includes activity counts for each minute of of the day, ranging from `r min(pull(accel, activity))` to `r max(pull(accel, activity))`. The mean activity count per minute during the study period was `r round(mean(pull(accel, counts)), digits = 2)`. 

#### Total activity per day
Traditional analyses of accelerometer data focus on the total activity over the day. Using your the tidied dataset, we will aggregate the data across minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r}
accel %>% 
  group_by(day_id) %>% 
  mutate(total_activity = sum(counts)) %>% 
  ungroup() %>% 
  select(week, day, total_activity) %>% 
  distinct() %>% 
  pivot_wider(
    names_from = "day",
    values_from = "total_activity"
  ) %>% 
  knitr::kable()
```

From this table, we can see that total activity seemed to decrease from week 1 to week 5 on the weekends.

#### 24-hour activity pattern by day
Accelerometer data allows the inspection of activity over the course of the day. Next we will make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week.

This first plot shows the individual data points with super-imposed geom_smooth lines.

```{r}
accel %>% 
  mutate(
    activity = as.numeric(as.character(activity)),
    activity = activity/60
    ) %>% 
  ggplot(aes(x = activity, y = counts, color = day)) + 
  geom_point(alpha = .5) +
  geom_smooth(aes(group = day),se = FALSE) +
  labs(
    title = "24-hour activity pattern by day of the week",
    x = "Hour",
    y = "Activity")
```

This second plot shows only the geom_smooth function and more clearly depicts the activity trends over the course of a day.
```{r}
accel %>% 
  mutate(
    activity = as.numeric(as.character(activity)),
    activity = activity/60
    ) %>% 
  ggplot(aes(x = activity, y = counts, color = day)) + 
  geom_smooth(se = FALSE) +
  labs(
    title = "24-hour activity pattern by day of the week",
    x = "Hour",
    y = "Activity")
```

Based on both plots, we can see that there are two peaks of activity. On Sundays, it appears that the patient was most active, or perhaps had scheduled exercise, around 10 am. The second activity peak occurred on Fridays, around 8-9 pm. The patient had the lowest activity on Saturdays.

