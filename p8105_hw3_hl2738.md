P8105 Homework 3
================
Heidi Lumish

## Problem 1

First we will load in the Instacart data from the p8105.datasets
library.

``` r
data("instacart")
```

#### Summary of the Instacart dataset

Next, we will summarize the dataset.

The Instacart dataset provides detailed information on grocery orders in
2017. The dataset has 15 variables, and it describes 1384617 products
from 131209 unique users. 59.86% products had been ordered by the user
in the past. The median time since the prior order for each customer was
15 (IQR 7, 30) days. There were 39123 unique products ordered from 134
different aisles.

### Specifics of the Instacart dataset

#### How many aisles are there, and which aisles are the most items ordered from?

There are 134 aisles, and the most items are ordered from aisle. Using
the following code chunk, we can determine that the most items are
ordered from the aisle fresh vegetables, fresh fruits, packaged
vegetables, yogurt, and packaged cheese.

``` r
instacart %>% 
  group_by(aisle) %>% 
  summarize(
    aisle_count = n()
  ) %>% 
  slice_max(aisle_count, n = 5)
```

    ## # A tibble: 5 × 2
    ##   aisle                      aisle_count
    ##   <chr>                            <int>
    ## 1 fresh vegetables                150609
    ## 2 fresh fruits                    150473
    ## 3 packaged vegetables fruits       78493
    ## 4 yogurt                           55240
    ## 5 packaged cheese                  41699

#### Plot showing the number of items ordered in each aisle

``` r
instacart %>% 
  group_by(aisle) %>% 
  summarize(
    aisle_count = n()
  ) %>% 
  filter(aisle_count >10000) %>% 
  ggplot(aes(x = reorder(aisle, aisle_count), y = aisle_count)) +
  geom_col(width = 0.7) +
  coord_flip() + 
  labs(
    title = "Number of products ordered per aisle",
    x = "Aisle name",
    y = "Number of products")
```

<img src="p8105_hw3_hl2738_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

#### Table of three most popular items

Make a table showing the three most popular items in each of the aisles
“baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
Include the number of times each item is ordered in your table.

``` r
instacart %>% 
  filter(
    aisle == "baking ingredients" |
      aisle == "dog food care" |
      aisle == "packaged vegetables fruits") %>%
  group_by(product_name) %>% 
  mutate(
    product_count = n()) %>%
  select(aisle, product_name, product_count) %>% 
  distinct() %>% 
  group_by(aisle) %>%
  slice_max(product_count, n = 3) %>% 
  mutate(
    Rank = order(product_count, decreasing = TRUE)
  ) %>%
  unite("product", product_name, product_count, sep = ": ") %>% 
  mutate(
    aisle = str_to_title(aisle)
  ) %>% 
  pivot_wider(
    names_from = "aisle",
    values_from = "product"
    ) %>% 
  knitr::kable(format = "simple",
               caption = "Top three most popular items by aisle")
```

| Rank | Baking Ingredients     | Dog Food Care                                     | Packaged Vegetables Fruits |
|-----:|:-----------------------|:--------------------------------------------------|:---------------------------|
|    1 | Light Brown Sugar: 499 | Snack Sticks Chicken & Rice Recipe Dog Treats: 30 | Organic Baby Spinach: 9784 |
|    2 | Pure Baking Soda: 387  | Organix Chicken & Brown Rice Recipe: 28           | Organic Raspberries: 5546  |
|    3 | Cane Sugar: 336        | Small Dog Biscuits: 26                            | Organic Blueberries: 4966  |

Top three most popular items by aisle

#### Pink Lady Apples and Coffee Ice Cream

The following is a table showing the mean hour of the day at which Pink
Lady Apples and Coffee Ice Cream are ordered on each day of the week.

``` r
instacart %>% 
  filter(
    product_name == "Pink Lady Apples" |
    product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(
    mean_hour = round(mean(order_hour_of_day), digits = 1)
  ) %>% 
  mutate(Day = case_when(
    order_dow == 0 ~ "Sunday",
    order_dow == 1 ~ "Monday",
    order_dow == 2 ~ "Tuesday",
    order_dow == 3 ~ "Wednesday",
    order_dow == 4 ~ "Thursday",
    order_dow == 5 ~ "Friday",
    order_dow == 6 ~ "Saturday")
  ) %>% 
  pivot_wider(
    names_from = "product_name",
    values_from = "mean_hour"
  ) %>% 
  select(-order_dow) %>% 
    knitr::kable(format = "simple",
                 caption = "Mean hour for ordering products by day of the week")
```

    ## `summarise()` has grouped output by 'product_name'. You can override using the `.groups` argument.

| Day       | Coffee Ice Cream | Pink Lady Apples |
|:----------|-----------------:|-----------------:|
| Sunday    |             13.8 |             13.4 |
| Monday    |             14.3 |             11.4 |
| Tuesday   |             15.4 |             11.7 |
| Wednesday |             15.3 |             14.2 |
| Thursday  |             15.2 |             11.6 |
| Friday    |             12.3 |             12.8 |
| Saturday  |             13.8 |             11.9 |

Mean hour for ordering products by day of the week

## Problem 2

#### Load and clean the BRFSS dataset

First we will load the BRFSS dataset from the p8105.datasets package.

``` r
data("brfss_smart2010")
```

Next we will clean the data.

``` r
brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  mutate(
    state = locationabbr,
    county = locationdesc
  ) %>% 
  select(-locationabbr, -locationdesc) %>% 
  relocate(year, state, county) %>% 
  filter(topic == "Overall Health") %>% 
  filter(response %in% c("Poor", "Fair", "Good", "Very good", "Excellent")) %>%
  mutate(
    response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))
  )
```

#### BRFSS dataset questions

###### Observed at 7 or more locations

In 2002, which states were observed at 7 or more locations? What about
in 2010?

Based on the code chunk below, we can see that the there were 6 states
(CT, FL, MA, NC, NJ, and PA) that were observed at 7 or more locations
in 2002. By comparison, in 2010, there 14 states (CA, CO, FL, MA, MD,
NC, NE, NJ, NY, OH, PA, SC, TX, and WA) that were observed at 7 or more
locations.

``` r
brfss %>% 
  filter(year == "2002") %>%
  select(state, county) %>% 
  distinct() %>% 
  group_by(state) %>% 
  summarize(count = n()) %>% 
  filter(count >= 7)
```

    ## # A tibble: 6 × 2
    ##   state count
    ##   <chr> <int>
    ## 1 CT        7
    ## 2 FL        7
    ## 3 MA        8
    ## 4 NC        7
    ## 5 NJ        8
    ## 6 PA       10

``` r
brfss %>% 
  filter(year == "2010") %>%
  select(state, county) %>% 
  distinct() %>% 
  group_by(state) %>% 
  summarize(count = n()) %>% 
  filter(count >= 7)
```

    ## # A tibble: 14 × 2
    ##    state count
    ##    <chr> <int>
    ##  1 CA       12
    ##  2 CO        7
    ##  3 FL       41
    ##  4 MA        9
    ##  5 MD       12
    ##  6 NC       12
    ##  7 NE       10
    ##  8 NJ       19
    ##  9 NY        9
    ## 10 OH        8
    ## 11 PA        7
    ## 12 SC        7
    ## 13 TX       16
    ## 14 WA       10

###### Excellent responses

Construct a dataset that is limited to *Excellent* responses, and
contains year, state, and a variable that averages the data\_value
across locations within a state. Make a “spaghetti” plot of this average
value over time within a state.

``` r
excellent = brfss %>% 
  filter(response == "Excellent") %>% 
  group_by(state, year) %>% 
  mutate(
    mean_data_value = mean(data_value)
  ) %>% 
  select(year, state, mean_data_value)

excellent %>% 
  ggplot(aes(x = year, y = mean_data_value, color = state)) +
  geom_point(size = .2) + 
  geom_line(aes(group = state), alpha = .3)
```

    ## Warning: Removed 71 rows containing missing values (geom_point).

    ## Warning: Removed 65 row(s) containing missing values (geom_path).

<img src="p8105_hw3_hl2738_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

###### Distribution of responses in 2006 and 2010 in NY state

Make a two-panel plot showing, for the years 2006, and 2010,
distribution of data\_value for responses (“Poor” to “Excellent”) among
locations in NY State.

``` r
brfss %>% 
  filter(
    year == c("2006", "2010"),
    state == "NY") %>% 
  ggplot(aes(x = data_value, fill = response)) +
  geom_density() + 
  facet_grid(.~year) +
  labs(
    title = "Distribution of data values by response in NY",
    x = "Data value",
    y = "Density")
```

    ## Warning in year == c("2006", "2010"): longer object length is not a multiple of
    ## shorter object length

<img src="p8105_hw3_hl2738_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />

This density plot shows that a response of “Poor” correlated with the
lowest data values, both in 2006 and 2010. A response of “Fair”
similarly correlated with lower data values, though not quite as low as
for “Poor.” In 2006, there was overlap between the data values for
“Good,” “Very good,” and “Excellent responses. In 2010, the highest data
values coresponsed with”Good" and “Very good” responses.

## Problem 3

This problem uses five weeks of accelerometer data collected on a 63
year-old male with BMI 25, who was admitted to the Advanced Cardiac Care
Center of Columbia University Medical Center and diagnosed with
congestive heart failure (CHF). The variables activity.\* are the
activity counts for each minute of a 24-hour day starting at midnight.

#### Load and tidy the accelerometer data

First we will load, tidy, and wrangle the accelerometer data.

``` r
accel = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    day_type = case_when(
      day == "Sunday" ~ "weekend",
      day == "Monday" ~ "weekday",
      day == "Tuesday" ~ "weekday",
      day == "Wednesday" ~ "weekday",
      day == "Thursday" ~ "weekday",
      day == "Friday" ~ "weekday",
      day == "Saturday" ~ "weekend"),
    week = as.numeric(as.character(week))
  ) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity",
    names_prefix = "activity_",
    values_to = "counts"
  ) %>% 
  mutate(counts = as.numeric(counts))
```

    ## Rows: 35 Columns: 1443

    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr    (1): day
    ## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...

    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

#### Summary of the dataset

Describe the resulting dataset (e.g. what variables exist, how many
observations, etc).

The tidied accelerometer dataset has 6 variables and 50400 observations.
It includes data from week 1 to week 5 of the study. The dataset
includes activity counts for each minute of 24-hour day. The mean
activity count per minute during the study period was 267.0440592.

#### Total activity per day

Traditional analyses of accelerometer data focus on the total activity
over the day. Using your tidied dataset, aggregate across minutes to
create a total activity variable for each day, and create a table
showing these totals. Are any trends apparent?

``` r
accel %>% 
  group_by(day_id) %>% 
  mutate(total_activity = sum(counts)) %>% 
  ungroup() %>% 
  select(week, day, total_activity) %>% 
  distinct() %>% 
  pivot_wider(
    names_from = "day",
    values_from = "total_activity"
  )
```

    ## # A tibble: 5 × 8
    ##    week  Friday  Monday Saturday Sunday Thursday Tuesday Wednesday
    ##   <dbl>   <dbl>   <dbl>    <dbl>  <dbl>    <dbl>   <dbl>     <dbl>
    ## 1     1 480543.  78828.   376254 631105  355924. 307094.   340115.
    ## 2     2 568839  295431    607175 422018  474048  423245    440962 
    ## 3     3 467420  685910    382928 467052  371230  381507    468869 
    ## 4     4 154049  409450      1440 260617  340291  319568    434460 
    ## 5     5 620860  389080      1440 138421  549658  367824    445366

#### 24-hour activity pattern by day

Accelerometer data allows the inspection activity over the course of the
day. Make a single-panel plot that shows the 24-hour activity time
courses for each day and use color to indicate day of the week. Describe
in words any patterns or conclusions you can make based on this graph.

``` r
accel %>% 
  mutate(
    activity = as.numeric(as.character(activity)),
    activity = activity/60
    ) %>% 
  ggplot(aes(x = activity, y = counts, color = day)) + 
  geom_point() +
  labs(
    title = "24-hour activity pattern by day of the week",
    x = "Hour",
    y = "Activity")
```

<img src="p8105_hw3_hl2738_files/figure-gfm/unnamed-chunk-13-1.png" width="90%" />

Based on this plot, we can see that there are two peaks of activity
during the day, around 10 am and 8 pm. The most activity tended to occur
on the weekends and on Fridays.
